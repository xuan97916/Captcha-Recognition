{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image,ImageEnhance,ImageFilter\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "import string\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文件名称加入文件名队列\n",
    "filename = []\n",
    "for i in range(10000):\n",
    "    filename.append('../dataset/train/'+str(i)+'.jpg')\n",
    "\n",
    "# 读入图片\n",
    "dataset = np.zeros(shape=(10000,80,100))  # 初始化图片存储数组\n",
    "for index,img in enumerate(filename):\n",
    "    im = Image.open(img).convert('L')  # 将图片灰度化读入\n",
    "    im = im.resize((100,80),Image.ANTIALIAS)  # 将图片缩放为相同大小（100*80）\n",
    "    enhancer = ImageEnhance.Contrast(im) #增加对比对\n",
    "    im = enhancer.enhance(2)\n",
    "    im = np.asarray(im, dtype=np.int64)\n",
    "    dataset[index] = im\n",
    "\n",
    "# 将图片灰度转换后，重新转化形状（单通道图片也需要单独一维）\n",
    "dataset = dataset.reshape(-1,80,100,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件中读入标签\n",
    "img_label = pd.read_csv('../dataset/train_labels.csv')\n",
    "img_label = img_label['y'].values.astype(str) # 和使用np.string_类型不同，待确定\n",
    "\n",
    "# 将标签转化为str类型，补0\n",
    "for index,label in enumerate(img_label):\n",
    "    if len(label) == 1:\n",
    "        img_label[index] = str('00')+label\n",
    "    elif len(label) == 2:\n",
    "        img_label[index] = str('0')+label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将标签进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldict = {'0':0,'1':1,'2':2,'3':3,'4':4,'5':5,'6':6,'7':7,'8':8,'9':9} # 定义标签字典\n",
    "num_classes = 10  # 确定数字类别数（共10类）\n",
    "\n",
    "# 将标签的每一个字符进行onehot-encoding\n",
    "y = []\n",
    "for i in range(len(img_label)):\n",
    "    c0 = keras.utils.to_categorical(labeldict[img_label[i][0]], num_classes)\n",
    "    c1 = keras.utils.to_categorical(labeldict[img_label[i][1]], num_classes)\n",
    "    c2 = keras.utils.to_categorical(labeldict[img_label[i][2]], num_classes)\n",
    "    c = np.concatenate((c0,c1,c2)) # 将三个onehot-encoding后的标签合并\n",
    "    y.append(c)\n",
    "y = np.array(y)\n",
    "X = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分验证集与训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整理channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n",
      "(9000, 80, 100, 1) (1000, 80, 100, 1)\n",
      "(9000, 80, 100, 1) (9000, 30)\n",
      "(1000, 80, 100, 1) (1000, 30)\n"
     ]
    }
   ],
   "source": [
    "# 由于keras调用tensorflow和Theano后端在池化层使用的通道顺序不同，需要分别对其进行转化\n",
    "\n",
    "img_rows = 80\n",
    "img_cols = 100\n",
    "print(K.image_data_format())\n",
    "print(x_train.shape,x_test.shape)\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 像素值归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = 255 - x_train\n",
    "x_test = 255 - x_test\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (9000, 80, 100, 1)\n",
      "9000 train samples\n",
      "1000 test samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设计神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())          \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes*3, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "9000/9000 [==============================] - 14s 2ms/step - loss: 0.2871 - acc: 0.9065 - val_loss: 0.1484 - val_acc: 0.9444\n",
      "Epoch 2/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0919 - acc: 0.9674 - val_loss: 0.0710 - val_acc: 0.9743\n",
      "Epoch 3/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0471 - acc: 0.9832 - val_loss: 0.0553 - val_acc: 0.9799\n",
      "Epoch 4/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0277 - acc: 0.9902 - val_loss: 0.0382 - val_acc: 0.9872\n",
      "Epoch 5/100\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.0170 - acc: 0.9939 - val_loss: 0.0313 - val_acc: 0.9899\n",
      "Epoch 6/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0118 - acc: 0.9958 - val_loss: 0.0292 - val_acc: 0.9901\n",
      "Epoch 7/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0087 - acc: 0.9969 - val_loss: 0.0205 - val_acc: 0.9936\n",
      "Epoch 8/100\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0229 - val_acc: 0.9933\n",
      "Epoch 9/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0197 - val_acc: 0.9943\n",
      "Epoch 10/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0157 - val_acc: 0.9953\n",
      "Epoch 11/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0190 - val_acc: 0.9947\n",
      "Epoch 12/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0164 - val_acc: 0.9953\n",
      "Epoch 13/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0148 - val_acc: 0.9955\n",
      "Epoch 14/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0183 - val_acc: 0.9949\n",
      "Epoch 15/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0144 - val_acc: 0.9960\n",
      "Epoch 16/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0149 - val_acc: 0.9958\n",
      "Epoch 17/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0168 - val_acc: 0.9959\n",
      "Epoch 18/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 8.6627e-04 - acc: 0.9998 - val_loss: 0.0153 - val_acc: 0.9961\n",
      "Epoch 19/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 7.4600e-04 - acc: 0.9998 - val_loss: 0.0151 - val_acc: 0.9961\n",
      "Epoch 20/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 6.9469e-04 - acc: 0.9998 - val_loss: 0.0151 - val_acc: 0.9964\n",
      "Epoch 21/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 6.9256e-04 - acc: 0.9998 - val_loss: 0.0161 - val_acc: 0.9960\n",
      "Epoch 22/100\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 6.7449e-04 - acc: 0.9998 - val_loss: 0.0153 - val_acc: 0.9962\n",
      "Epoch 23/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 6.1837e-04 - acc: 0.9998 - val_loss: 0.0133 - val_acc: 0.9968\n",
      "Epoch 24/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 5.1438e-04 - acc: 0.9999 - val_loss: 0.0132 - val_acc: 0.9967\n",
      "Epoch 25/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 4.3058e-04 - acc: 0.9999 - val_loss: 0.0120 - val_acc: 0.9968\n",
      "Epoch 26/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 4.3780e-04 - acc: 0.9999 - val_loss: 0.0158 - val_acc: 0.9958\n",
      "Epoch 27/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 5.4481e-04 - acc: 0.9998 - val_loss: 0.0159 - val_acc: 0.9960\n",
      "Epoch 28/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 5.7966e-04 - acc: 0.9998 - val_loss: 0.0163 - val_acc: 0.9959\n",
      "Epoch 29/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 5.4345e-04 - acc: 0.9999 - val_loss: 0.0132 - val_acc: 0.9967\n",
      "Epoch 30/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 4.3939e-04 - acc: 0.9999 - val_loss: 0.0150 - val_acc: 0.9964\n",
      "Epoch 31/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 4.5449e-04 - acc: 0.9999 - val_loss: 0.0136 - val_acc: 0.9963\n",
      "Epoch 32/100\n",
      "9000/9000 [==============================] - 13s 1ms/step - loss: 3.2903e-04 - acc: 0.9999 - val_loss: 0.0142 - val_acc: 0.9959\n",
      "Epoch 33/100\n",
      "9000/9000 [==============================] - 12s 1ms/step - loss: 2.8922e-04 - acc: 0.9999 - val_loss: 0.0149 - val_acc: 0.9962\n",
      "Epoch 00033: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea606b45c0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "# 设置early_stopping回调函数，当验证集acc不再升高时停止训练\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=10, verbose=2,mode='max')\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model.save('my_model milestone 3-12.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = []\n",
    "for i in range(20000):\n",
    "    filename.append('../dataset/test/'+str(i)+'.jpg')\n",
    "test_dataset = np.zeros(shape=(20000,80,100))  # 存储图片的数组\n",
    "for index,img in enumerate(filename):\n",
    "    im = Image.open(img).convert('L')  # 将图片灰度化读入\n",
    "    im = im.resize((100,80),Image.ANTIALIAS)  # 将图片缩放为相同大小（100*80）\n",
    "    enhancer = ImageEnhance.Contrast(im) #增加对比对\n",
    "    im = enhancer.enhance(2)\n",
    "    im = np.asarray(im, dtype=np.int64)\n",
    "    test_dataset[index] = im\n",
    "\n",
    "\n",
    "test_dataset = test_dataset.reshape(-1,80,100,1) # 将图片灰度转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n",
      "(20000, 80, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "img_rows = 80\n",
    "img_cols = 100\n",
    "print(K.image_data_format())\n",
    "print(test_dataset.shape)\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    test_dataset = test_dataset.reshape(test_dataset.shape[0], 1, img_rows, img_cols)\n",
    "else:\n",
    "    test_dataset = test_dataset.reshape(test_dataset.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "test_dataset = 255 - test_dataset\n",
    "test_dataset = test_dataset.astype('float32')\n",
    "test_dataset /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用模型对测试集进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_dataset,batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdict = ['0','1','2','3','4','5','6','7','8','9']\n",
    "pred_result = []\n",
    "for i in range(pred.shape[0]):\n",
    "    c0 = outdict[np.argmax(pred[i][:10])]\n",
    "    c1 = outdict[np.argmax(pred[i][10:20])]\n",
    "    c2 = outdict[np.argmax(pred[i][20:30])]\n",
    "    c = c0+c1+c2\n",
    "    pred_result.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = np.array(pred_result).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = pd.DataFrame(pred_result,dtype='int64')\n",
    "pred_result.columns = ['y']\n",
    "\n",
    "# test_pre\n",
    "test_id = pd.DataFrame(np.arange(20000))\n",
    "test_id.columns = ['id']\n",
    "\n",
    "test_id['y'] = pred_result['y']\n",
    "test_id\n",
    "\n",
    "test_id.to_csv('milestone3-12 Nov 24.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8355"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "x = pd.read_csv('../dataset/corr_result.csv')\n",
    "x = x['y']\n",
    "x = np.array(x)\n",
    "x = x[:4000]\n",
    "\n",
    "y = pd.read_csv('./milestone3-12 Nov 24.csv')\n",
    "y = y['y']\n",
    "y = np.array(y)\n",
    "y = y[:4000]\n",
    "\n",
    "acc = accuracy_score(x,y)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
